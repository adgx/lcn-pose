{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# prompt: Check the version of tensorflow installed\n\nimport tensorflow as tf\ntf.__version__\nimport sys\nsys.version","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-23T07:13:35.870081Z","iopub.execute_input":"2025-08-23T07:13:35.870342Z","iopub.status.idle":"2025-08-23T07:13:35.876363Z","shell.execute_reply.started":"2025-08-23T07:13:35.870321Z","shell.execute_reply":"2025-08-23T07:13:35.875547Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]'"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"!rm -rf lcn-pose  # Elimina la cartella esistente\n!git clone \"https://github.com/adgx/lcn-pose/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T07:13:35.877720Z","iopub.execute_input":"2025-08-23T07:13:35.877997Z","iopub.status.idle":"2025-08-23T07:13:38.790519Z","shell.execute_reply.started":"2025-08-23T07:13:35.877973Z","shell.execute_reply":"2025-08-23T07:13:38.789762Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'lcn-pose'...\nremote: Enumerating objects: 866, done.\u001b[K\nremote: Counting objects: 100% (134/134), done.\u001b[K\nremote: Compressing objects: 100% (103/103), done.\u001b[K\nremote: Total 866 (delta 45), reused 86 (delta 23), pack-reused 732 (from 1)\u001b[K\nReceiving objects: 100% (866/866), 16.98 MiB | 38.38 MiB/s, done.\nResolving deltas: 100% (502/502), done.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nos.chdir(\"/kaggle/working/lcn-pose/\")\n!mkdir -p \"dataset\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T07:13:38.791572Z","iopub.execute_input":"2025-08-23T07:13:38.791905Z","iopub.status.idle":"2025-08-23T07:13:38.923356Z","shell.execute_reply.started":"2025-08-23T07:13:38.791873Z","shell.execute_reply":"2025-08-23T07:13:38.922482Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Scarichi il dataset\n!gdown 1Ku9lKq6pmcdDyO1lGKJyPS4Ykow9wWuI","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T07:13:38.924388Z","iopub.execute_input":"2025-08-23T07:13:38.924673Z","iopub.status.idle":"2025-08-23T07:14:02.964408Z","shell.execute_reply.started":"2025-08-23T07:13:38.924640Z","shell.execute_reply":"2025-08-23T07:14:02.963471Z"}},"outputs":[{"name":"stdout","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1Ku9lKq6pmcdDyO1lGKJyPS4Ykow9wWuI\nFrom (redirected): https://drive.google.com/uc?id=1Ku9lKq6pmcdDyO1lGKJyPS4Ykow9wWuI&confirm=t&uuid=dd7fde46-82d1-481d-938c-39d444e8db20\nTo: /kaggle/working/lcn-pose/h3.6m.zip\n100%|██████████████████████████████████████| 1.93G/1.93G [00:20<00:00, 93.6MB/s]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import zipfile\nwith zipfile.ZipFile(\"/kaggle/working/lcn-pose/h3.6m.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(\"/kaggle/working/lcn-pose/dataset\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T07:14:02.966467Z","iopub.execute_input":"2025-08-23T07:14:02.966706Z","iopub.status.idle":"2025-08-23T07:14:21.085757Z","shell.execute_reply.started":"2025-08-23T07:14:02.966684Z","shell.execute_reply":"2025-08-23T07:14:21.084888Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import os\nos.rename(\"/kaggle/working/lcn-pose/dataset/h36m_test.pkl\", \"/kaggle/working/lcn-pose/dataset/h36m_val.pkl\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T07:14:21.086716Z","iopub.execute_input":"2025-08-23T07:14:21.087039Z","iopub.status.idle":"2025-08-23T07:14:21.091715Z","shell.execute_reply.started":"2025-08-23T07:14:21.087014Z","shell.execute_reply":"2025-08-23T07:14:21.090986Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import tensorflow as tf\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T07:14:21.092642Z","iopub.execute_input":"2025-08-23T07:14:21.092944Z","iopub.status.idle":"2025-08-23T07:14:21.933796Z","shell.execute_reply.started":"2025-08-23T07:14:21.092920Z","shell.execute_reply":"2025-08-23T07:14:21.932779Z"}},"outputs":[{"name":"stdout","text":"Num GPUs Available:  1\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"TRAIN_SET = \"h36m\"\nVALIDATION_SET = \"h36m\"\nTEST_SET = \"h36m\"\nMASK_TYPE = \"locally_connected\"\nF = 64\nEPOCH = 200\nREGULARIZATION = 0.000\nLEARNING_RATE = 0.001\n#LAYERS = 3\n#KNN = 3\nTEST_INDICES = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T07:14:21.934837Z","iopub.execute_input":"2025-08-23T07:14:21.935140Z","iopub.status.idle":"2025-08-23T07:14:26.442127Z","shell.execute_reply.started":"2025-08-23T07:14:21.935112Z","shell.execute_reply":"2025-08-23T07:14:26.441244Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"!python train.py --train_set={TRAIN_SET} --validation_set={VALIDATION_SET} --mask-type {MASK_TYPE} --channels {F} --epoch={EPOCH}  --learning_rate {LEARNING_RATE} --test-indices {TEST_INDICES} --regularization {REGULARIZATION} ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T07:14:26.443183Z","iopub.execute_input":"2025-08-23T07:14:26.443476Z","iopub.status.idle":"2025-08-23T11:35:17.922837Z","shell.execute_reply.started":"2025-08-23T07:14:26.443450Z","shell.execute_reply":"2025-08-23T11:35:17.921804Z"}},"outputs":[{"name":"stdout","text":"2025-08-23 07:14:28.783654: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755933268.812711     116 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755933268.820132     116 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nloading h36m_train.pkl\nloading h36m_val.pkl\n{'F': 64,\n 'batch_norm': True,\n 'batch_size': 200,\n 'checkpoints': 'final',\n 'decay_params': {'decay_rate': 0.96, 'decay_steps': 32000},\n 'decay_type': 'exp',\n 'dir_name': 'test1/',\n 'dropout': 0,\n 'in_F': 2,\n 'in_joints': 17,\n 'init_type': 'same',\n 'is_training': True,\n 'knn': 2,\n 'learning_rate': 0.001,\n 'mask_type': 'locally_connected',\n 'max_norm': True,\n 'neighbour_matrix': array([[1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n        0.],\n       [1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n        0.],\n       [1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0.],\n       [0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0.],\n       [1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n        0.],\n       [1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0.],\n       [0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n        0.],\n       [1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n        0.],\n       [1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n        0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n        0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n        0.],\n       [1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n        0.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0.,\n        0.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n        0.],\n       [1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1.,\n        1.],\n       [0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1.,\n        1.],\n       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n        1.]], dtype=float32),\n 'num_epochs': 200,\n 'num_layers': 3,\n 'out_joints': 17,\n 'regularization': 0.0,\n 'residual': True}\nI0000 00:00:1755933373.333725     116 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1755933373.377260     116 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\nTotal steps to be done to complete all the epochs: 1559752\nstep 7798 / 1559752 (epoch 1.00 / 200):\n  learning_rate = 9.90e-04, loss_average = 6.9469e-04\nvalidation loss: 2.0262e-03\ntime: 67s (wall 77s)\nstep 15596 / 1559752 (epoch 2.00 / 200):\n  learning_rate = 9.80e-04, loss_average = 4.5749e-04\nvalidation loss: 2.2004e-03\ntime: 137s (wall 155s)\nstep 23394 / 1559752 (epoch 3.00 / 200):\n  learning_rate = 9.71e-04, loss_average = 3.7363e-04\nvalidation loss: 2.1111e-03\ntime: 204s (wall 230s)\nstep 31192 / 1559752 (epoch 4.00 / 200):\n  learning_rate = 9.61e-04, loss_average = 2.8594e-04\nvalidation loss: 1.9356e-03\ntime: 271s (wall 307s)\nstep 38990 / 1559752 (epoch 5.00 / 200):\n  learning_rate = 9.51e-04, loss_average = 2.4763e-04\nvalidation loss: 1.8134e-03\ntime: 341s (wall 385s)\nstep 46788 / 1559752 (epoch 6.00 / 200):\n  learning_rate = 9.42e-04, loss_average = 2.1397e-04\nvalidation loss: 1.7710e-03\ntime: 408s (wall 461s)\nstep 54586 / 1559752 (epoch 7.00 / 200):\n  learning_rate = 9.33e-04, loss_average = 1.7840e-04\nvalidation loss: 1.6346e-03\ntime: 477s (wall 538s)\nstep 62384 / 1559752 (epoch 8.00 / 200):\n  learning_rate = 9.24e-04, loss_average = 1.6460e-04\nvalidation loss: 1.6295e-03\ntime: 546s (wall 616s)\nstep 70182 / 1559752 (epoch 9.00 / 200):\n  learning_rate = 9.14e-04, loss_average = 1.2852e-04\nvalidation loss: 1.5021e-03\ntime: 615s (wall 694s)\nstep 77980 / 1559752 (epoch 10.00 / 200):\n  learning_rate = 9.05e-04, loss_average = 1.2984e-04\nvalidation loss: 1.5446e-03\ntime: 684s (wall 772s)\nstep 85778 / 1559752 (epoch 11.00 / 200):\n  learning_rate = 8.96e-04, loss_average = 1.1495e-04\nvalidation loss: 1.4545e-03\ntime: 750s (wall 846s)\nstep 93576 / 1559752 (epoch 12.00 / 200):\n  learning_rate = 8.87e-04, loss_average = 1.1097e-04\nvalidation loss: 1.4613e-03\ntime: 820s (wall 924s)\nstep 101374 / 1559752 (epoch 13.00 / 200):\n  learning_rate = 8.79e-04, loss_average = 1.0027e-04\nvalidation loss: 1.4062e-03\ntime: 887s (wall 1001s)\nstep 109172 / 1559752 (epoch 14.00 / 200):\n  learning_rate = 8.70e-04, loss_average = 9.8489e-05\nvalidation loss: 1.3852e-03\ntime: 954s (wall 1077s)\nstep 116970 / 1559752 (epoch 15.00 / 200):\n  learning_rate = 8.61e-04, loss_average = 9.0913e-05\nvalidation loss: 1.3621e-03\ntime: 1024s (wall 1155s)\nstep 124768 / 1559752 (epoch 16.00 / 200):\n  learning_rate = 8.53e-04, loss_average = 8.6316e-05\nvalidation loss: 1.3263e-03\ntime: 1093s (wall 1233s)\nstep 132566 / 1559752 (epoch 17.00 / 200):\n  learning_rate = 8.44e-04, loss_average = 8.9368e-05\nvalidation loss: 1.2922e-03\ntime: 1162s (wall 1310s)\nstep 140364 / 1559752 (epoch 18.00 / 200):\n  learning_rate = 8.36e-04, loss_average = 7.8288e-05\nvalidation loss: 1.2929e-03\ntime: 1231s (wall 1388s)\nstep 148162 / 1559752 (epoch 19.00 / 200):\n  learning_rate = 8.28e-04, loss_average = 7.9062e-05\nvalidation loss: 1.2695e-03\ntime: 1297s (wall 1463s)\nstep 155960 / 1559752 (epoch 20.00 / 200):\n  learning_rate = 8.20e-04, loss_average = 6.8340e-05\nvalidation loss: 1.2546e-03\ntime: 1366s (wall 1541s)\nstep 163758 / 1559752 (epoch 21.00 / 200):\n  learning_rate = 8.11e-04, loss_average = 7.1957e-05\nvalidation loss: 1.1902e-03\ntime: 1435s (wall 1618s)\nstep 171556 / 1559752 (epoch 22.00 / 200):\n  learning_rate = 8.03e-04, loss_average = 7.0859e-05\nvalidation loss: 1.2365e-03\ntime: 1504s (wall 1696s)\nstep 179354 / 1559752 (epoch 23.00 / 200):\n  learning_rate = 7.95e-04, loss_average = 6.6301e-05\nvalidation loss: 1.1977e-03\ntime: 1570s (wall 1771s)\nstep 187152 / 1559752 (epoch 24.00 / 200):\n  learning_rate = 7.88e-04, loss_average = 6.3883e-05\nvalidation loss: 1.1926e-03\ntime: 1638s (wall 1848s)\nstep 194950 / 1559752 (epoch 25.00 / 200):\n  learning_rate = 7.80e-04, loss_average = 6.3207e-05\nvalidation loss: 1.1526e-03\ntime: 1705s (wall 1922s)\nstep 202748 / 1559752 (epoch 26.00 / 200):\n  learning_rate = 7.72e-04, loss_average = 6.2264e-05\nvalidation loss: 1.1639e-03\ntime: 1774s (wall 2000s)\nstep 210546 / 1559752 (epoch 27.00 / 200):\n  learning_rate = 7.64e-04, loss_average = 5.6492e-05\nvalidation loss: 1.1391e-03\ntime: 1842s (wall 2077s)\nstep 218344 / 1559752 (epoch 28.00 / 200):\n  learning_rate = 7.57e-04, loss_average = 6.1617e-05\nvalidation loss: 1.0907e-03\ntime: 1909s (wall 2153s)\nstep 226142 / 1559752 (epoch 29.00 / 200):\n  learning_rate = 7.49e-04, loss_average = 5.6051e-05\nvalidation loss: 1.1236e-03\ntime: 1978s (wall 2231s)\nstep 233940 / 1559752 (epoch 30.00 / 200):\n  learning_rate = 7.42e-04, loss_average = 5.6601e-05\nvalidation loss: 1.1179e-03\ntime: 2046s (wall 2307s)\nstep 241738 / 1559752 (epoch 31.00 / 200):\n  learning_rate = 7.35e-04, loss_average = 5.3536e-05\nvalidation loss: 1.0768e-03\ntime: 2112s (wall 2382s)\nstep 249536 / 1559752 (epoch 32.00 / 200):\n  learning_rate = 7.27e-04, loss_average = 5.2899e-05\nvalidation loss: 1.0653e-03\ntime: 2181s (wall 2460s)\nstep 257334 / 1559752 (epoch 33.00 / 200):\n  learning_rate = 7.20e-04, loss_average = 4.8905e-05\nvalidation loss: 1.0939e-03\ntime: 2250s (wall 2537s)\nstep 265132 / 1559752 (epoch 34.00 / 200):\n  learning_rate = 7.13e-04, loss_average = 5.1675e-05\nvalidation loss: 1.0929e-03\ntime: 2316s (wall 2612s)\nstep 272930 / 1559752 (epoch 35.00 / 200):\n  learning_rate = 7.06e-04, loss_average = 4.9213e-05\nvalidation loss: 1.0983e-03\ntime: 2384s (wall 2689s)\nstep 280728 / 1559752 (epoch 36.00 / 200):\n  learning_rate = 6.99e-04, loss_average = 4.8419e-05\nvalidation loss: 1.0664e-03\ntime: 2450s (wall 2764s)\nstep 288526 / 1559752 (epoch 37.00 / 200):\n  learning_rate = 6.92e-04, loss_average = 4.9220e-05\nvalidation loss: 1.0751e-03\ntime: 2518s (wall 2840s)\nstep 296324 / 1559752 (epoch 38.00 / 200):\n  learning_rate = 6.85e-04, loss_average = 4.6214e-05\nvalidation loss: 1.0383e-03\ntime: 2584s (wall 2915s)\nstep 304122 / 1559752 (epoch 39.00 / 200):\n  learning_rate = 6.78e-04, loss_average = 4.5536e-05\nvalidation loss: 1.0520e-03\ntime: 2653s (wall 2993s)\nstep 311920 / 1559752 (epoch 40.00 / 200):\n  learning_rate = 6.72e-04, loss_average = 4.8319e-05\nvalidation loss: 1.0711e-03\ntime: 2720s (wall 3068s)\nstep 319718 / 1559752 (epoch 41.00 / 200):\n  learning_rate = 6.65e-04, loss_average = 4.5693e-05\nvalidation loss: 1.0203e-03\ntime: 2788s (wall 3144s)\nstep 327516 / 1559752 (epoch 42.00 / 200):\n  learning_rate = 6.58e-04, loss_average = 4.6210e-05\nvalidation loss: 1.0304e-03\ntime: 2856s (wall 3222s)\nstep 335314 / 1559752 (epoch 43.00 / 200):\n  learning_rate = 6.52e-04, loss_average = 4.3570e-05\nvalidation loss: 1.0165e-03\ntime: 2922s (wall 3297s)\nstep 343112 / 1559752 (epoch 44.00 / 200):\n  learning_rate = 6.46e-04, loss_average = 4.2761e-05\nvalidation loss: 1.0307e-03\ntime: 2992s (wall 3375s)\nstep 350910 / 1559752 (epoch 45.00 / 200):\n  learning_rate = 6.39e-04, loss_average = 4.1461e-05\nvalidation loss: 9.9232e-04\ntime: 3058s (wall 3450s)\nstep 358708 / 1559752 (epoch 46.00 / 200):\n  learning_rate = 6.33e-04, loss_average = 4.0934e-05\nvalidation loss: 1.0078e-03\ntime: 3127s (wall 3527s)\nstep 366506 / 1559752 (epoch 47.00 / 200):\n  learning_rate = 6.27e-04, loss_average = 4.1591e-05\nvalidation loss: 1.0099e-03\ntime: 3196s (wall 3604s)\nstep 374304 / 1559752 (epoch 48.00 / 200):\n  learning_rate = 6.20e-04, loss_average = 3.9130e-05\nvalidation loss: 9.9213e-04\ntime: 3262s (wall 3680s)\nstep 382102 / 1559752 (epoch 49.00 / 200):\n  learning_rate = 6.14e-04, loss_average = 3.7631e-05\nvalidation loss: 1.0090e-03\ntime: 3332s (wall 3758s)\nstep 389900 / 1559752 (epoch 50.00 / 200):\n  learning_rate = 6.08e-04, loss_average = 3.8360e-05\nvalidation loss: 9.8346e-04\ntime: 3398s (wall 3832s)\nstep 397698 / 1559752 (epoch 51.00 / 200):\n  learning_rate = 6.02e-04, loss_average = 3.6333e-05\nvalidation loss: 9.7344e-04\ntime: 3467s (wall 3910s)\nstep 405496 / 1559752 (epoch 51.99 / 200):\n  learning_rate = 5.96e-04, loss_average = 3.7513e-05\nvalidation loss: 9.9468e-04\ntime: 3536s (wall 3988s)\nstep 413294 / 1559752 (epoch 52.99 / 200):\n  learning_rate = 5.90e-04, loss_average = 3.6744e-05\nvalidation loss: 9.7571e-04\ntime: 3604s (wall 4065s)\nstep 421092 / 1559752 (epoch 53.99 / 200):\n  learning_rate = 5.84e-04, loss_average = 3.6559e-05\nvalidation loss: 9.5548e-04\ntime: 3670s (wall 4140s)\nstep 428890 / 1559752 (epoch 54.99 / 200):\n  learning_rate = 5.79e-04, loss_average = 3.8549e-05\nvalidation loss: 9.5691e-04\ntime: 3739s (wall 4217s)\nstep 436688 / 1559752 (epoch 55.99 / 200):\n  learning_rate = 5.73e-04, loss_average = 3.5855e-05\nvalidation loss: 9.7010e-04\ntime: 3805s (wall 4292s)\nstep 444486 / 1559752 (epoch 56.99 / 200):\n  learning_rate = 5.67e-04, loss_average = 3.4387e-05\nvalidation loss: 9.6704e-04\ntime: 3873s (wall 4369s)\nstep 452284 / 1559752 (epoch 57.99 / 200):\n  learning_rate = 5.62e-04, loss_average = 3.7620e-05\nvalidation loss: 9.6844e-04\ntime: 3939s (wall 4443s)\nstep 460082 / 1559752 (epoch 58.99 / 200):\n  learning_rate = 5.56e-04, loss_average = 3.3637e-05\nvalidation loss: 9.5107e-04\ntime: 4007s (wall 4520s)\nstep 467880 / 1559752 (epoch 59.99 / 200):\n  learning_rate = 5.51e-04, loss_average = 3.3511e-05\nvalidation loss: 9.5960e-04\ntime: 4075s (wall 4598s)\nstep 475678 / 1559752 (epoch 60.99 / 200):\n  learning_rate = 5.45e-04, loss_average = 3.3086e-05\nvalidation loss: 9.1177e-04\ntime: 4141s (wall 4672s)\nstep 483476 / 1559752 (epoch 61.99 / 200):\n  learning_rate = 5.40e-04, loss_average = 3.1983e-05\nvalidation loss: 9.4061e-04\ntime: 4210s (wall 4750s)\nstep 491274 / 1559752 (epoch 62.99 / 200):\n  learning_rate = 5.34e-04, loss_average = 3.2448e-05\nvalidation loss: 9.4378e-04\ntime: 4276s (wall 4825s)\nstep 499072 / 1559752 (epoch 63.99 / 200):\n  learning_rate = 5.29e-04, loss_average = 3.2487e-05\nvalidation loss: 8.9386e-04\ntime: 4344s (wall 4901s)\nstep 506870 / 1559752 (epoch 64.99 / 200):\n  learning_rate = 5.24e-04, loss_average = 3.3788e-05\nvalidation loss: 9.2908e-04\ntime: 4413s (wall 4979s)\nstep 514668 / 1559752 (epoch 65.99 / 200):\n  learning_rate = 5.19e-04, loss_average = 3.3072e-05\nvalidation loss: 8.9740e-04\ntime: 4479s (wall 5054s)\nstep 522466 / 1559752 (epoch 66.99 / 200):\n  learning_rate = 5.14e-04, loss_average = 3.1998e-05\nvalidation loss: 9.2608e-04\ntime: 4545s (wall 5129s)\nstep 530264 / 1559752 (epoch 67.99 / 200):\n  learning_rate = 5.08e-04, loss_average = 3.0272e-05\nvalidation loss: 8.9027e-04\ntime: 4613s (wall 5205s)\nstep 538062 / 1559752 (epoch 68.99 / 200):\n  learning_rate = 5.03e-04, loss_average = 3.0854e-05\nvalidation loss: 8.9813e-04\ntime: 4682s (wall 5283s)\nstep 545860 / 1559752 (epoch 69.99 / 200):\n  learning_rate = 4.98e-04, loss_average = 2.9448e-05\nvalidation loss: 9.0802e-04\ntime: 4748s (wall 5358s)\nstep 553658 / 1559752 (epoch 70.99 / 200):\n  learning_rate = 4.93e-04, loss_average = 2.8412e-05\nvalidation loss: 8.8604e-04\ntime: 4816s (wall 5434s)\nstep 561456 / 1559752 (epoch 71.99 / 200):\n  learning_rate = 4.89e-04, loss_average = 2.9307e-05\nvalidation loss: 8.8633e-04\ntime: 4885s (wall 5512s)\nstep 569254 / 1559752 (epoch 72.99 / 200):\n  learning_rate = 4.84e-04, loss_average = 2.9292e-05\nvalidation loss: 8.7353e-04\ntime: 4953s (wall 5588s)\nstep 577052 / 1559752 (epoch 73.99 / 200):\n  learning_rate = 4.79e-04, loss_average = 2.7859e-05\nvalidation loss: 8.6184e-04\ntime: 5023s (wall 5666s)\nstep 584850 / 1559752 (epoch 74.99 / 200):\n  learning_rate = 4.74e-04, loss_average = 2.9854e-05\nvalidation loss: 8.8288e-04\ntime: 5093s (wall 5745s)\nstep 592648 / 1559752 (epoch 75.99 / 200):\n  learning_rate = 4.70e-04, loss_average = 2.8652e-05\nvalidation loss: 8.7603e-04\ntime: 5161s (wall 5820s)\nstep 600446 / 1559752 (epoch 76.99 / 200):\n  learning_rate = 4.65e-04, loss_average = 2.8260e-05\nvalidation loss: 8.8998e-04\ntime: 5230s (wall 5898s)\nstep 608244 / 1559752 (epoch 77.99 / 200):\n  learning_rate = 4.60e-04, loss_average = 2.6993e-05\nvalidation loss: 8.8225e-04\ntime: 5297s (wall 5973s)\nstep 616042 / 1559752 (epoch 78.99 / 200):\n  learning_rate = 4.56e-04, loss_average = 2.8942e-05\nvalidation loss: 8.5048e-04\ntime: 5366s (wall 6050s)\nstep 623840 / 1559752 (epoch 79.99 / 200):\n  learning_rate = 4.51e-04, loss_average = 2.6835e-05\nvalidation loss: 8.6201e-04\ntime: 5435s (wall 6127s)\nstep 631638 / 1559752 (epoch 80.99 / 200):\n  learning_rate = 4.47e-04, loss_average = 2.7198e-05\nvalidation loss: 8.7832e-04\ntime: 5504s (wall 6205s)\nstep 639436 / 1559752 (epoch 81.99 / 200):\n  learning_rate = 4.42e-04, loss_average = 2.5828e-05\nvalidation loss: 8.6074e-04\ntime: 5572s (wall 6280s)\nstep 647234 / 1559752 (epoch 82.99 / 200):\n  learning_rate = 4.38e-04, loss_average = 2.7371e-05\nvalidation loss: 8.5712e-04\ntime: 5641s (wall 6357s)\nstep 655032 / 1559752 (epoch 83.99 / 200):\n  learning_rate = 4.34e-04, loss_average = 2.5668e-05\nvalidation loss: 8.3047e-04\ntime: 5708s (wall 6433s)\nstep 662830 / 1559752 (epoch 84.99 / 200):\n  learning_rate = 4.29e-04, loss_average = 2.5505e-05\nvalidation loss: 8.2947e-04\ntime: 5779s (wall 6512s)\nstep 670628 / 1559752 (epoch 85.99 / 200):\n  learning_rate = 4.25e-04, loss_average = 2.5353e-05\nvalidation loss: 8.4119e-04\ntime: 5856s (wall 6593s)\nstep 678426 / 1559752 (epoch 86.99 / 200):\n  learning_rate = 4.21e-04, loss_average = 2.5582e-05\nvalidation loss: 8.2827e-04\ntime: 5934s (wall 6673s)\nstep 686224 / 1559752 (epoch 87.99 / 200):\n  learning_rate = 4.17e-04, loss_average = 2.5035e-05\nvalidation loss: 8.1897e-04\ntime: 6012s (wall 6755s)\nstep 694022 / 1559752 (epoch 88.99 / 200):\n  learning_rate = 4.13e-04, loss_average = 2.5946e-05\nvalidation loss: 8.3125e-04\ntime: 6086s (wall 6835s)\nstep 701820 / 1559752 (epoch 89.99 / 200):\n  learning_rate = 4.08e-04, loss_average = 2.5669e-05\nvalidation loss: 8.2164e-04\ntime: 6159s (wall 6912s)\nstep 709618 / 1559752 (epoch 90.99 / 200):\n  learning_rate = 4.04e-04, loss_average = 2.5623e-05\nvalidation loss: 8.1193e-04\ntime: 6232s (wall 6991s)\nstep 717416 / 1559752 (epoch 91.99 / 200):\n  learning_rate = 4.00e-04, loss_average = 2.3949e-05\nvalidation loss: 8.1626e-04\ntime: 6308s (wall 7072s)\nstep 725214 / 1559752 (epoch 92.99 / 200):\n  learning_rate = 3.96e-04, loss_average = 2.5162e-05\nvalidation loss: 8.1697e-04\ntime: 6380s (wall 7149s)\nstep 733012 / 1559752 (epoch 93.99 / 200):\n  learning_rate = 3.93e-04, loss_average = 2.3973e-05\nvalidation loss: 8.1726e-04\ntime: 6453s (wall 7228s)\nstep 740810 / 1559752 (epoch 94.99 / 200):\n  learning_rate = 3.89e-04, loss_average = 2.4216e-05\nvalidation loss: 8.0982e-04\ntime: 6523s (wall 7304s)\nstep 748608 / 1559752 (epoch 95.99 / 200):\n  learning_rate = 3.85e-04, loss_average = 2.4212e-05\nvalidation loss: 8.0389e-04\ntime: 6595s (wall 7382s)\nstep 756406 / 1559752 (epoch 96.99 / 200):\n  learning_rate = 3.81e-04, loss_average = 2.3407e-05\nvalidation loss: 8.2345e-04\ntime: 6667s (wall 7461s)\nstep 764204 / 1559752 (epoch 97.99 / 200):\n  learning_rate = 3.77e-04, loss_average = 2.3083e-05\nvalidation loss: 7.9297e-04\ntime: 6736s (wall 7536s)\nstep 772002 / 1559752 (epoch 98.99 / 200):\n  learning_rate = 3.74e-04, loss_average = 2.3059e-05\nvalidation loss: 7.9746e-04\ntime: 6813s (wall 7617s)\nstep 779800 / 1559752 (epoch 99.99 / 200):\n  learning_rate = 3.70e-04, loss_average = 2.2767e-05\nvalidation loss: 7.8268e-04\ntime: 6889s (wall 7696s)\nstep 787598 / 1559752 (epoch 100.99 / 200):\n  learning_rate = 3.66e-04, loss_average = 2.2614e-05\nvalidation loss: 8.0644e-04\ntime: 6964s (wall 7775s)\nstep 795396 / 1559752 (epoch 101.99 / 200):\n  learning_rate = 3.63e-04, loss_average = 2.3424e-05\nvalidation loss: 7.9101e-04\ntime: 7039s (wall 7854s)\nstep 803194 / 1559752 (epoch 102.99 / 200):\n  learning_rate = 3.59e-04, loss_average = 2.1332e-05\nvalidation loss: 7.9046e-04\ntime: 7113s (wall 7931s)\nstep 810992 / 1559752 (epoch 103.99 / 200):\n  learning_rate = 3.55e-04, loss_average = 2.2651e-05\nvalidation loss: 7.7978e-04\ntime: 7189s (wall 8011s)\nstep 818790 / 1559752 (epoch 104.99 / 200):\n  learning_rate = 3.52e-04, loss_average = 2.1466e-05\nvalidation loss: 7.8707e-04\ntime: 7266s (wall 8091s)\nstep 826588 / 1559752 (epoch 105.99 / 200):\n  learning_rate = 3.48e-04, loss_average = 2.2036e-05\nvalidation loss: 7.6790e-04\ntime: 7340s (wall 8168s)\nstep 834386 / 1559752 (epoch 106.99 / 200):\n  learning_rate = 3.45e-04, loss_average = 2.2801e-05\nvalidation loss: 7.8839e-04\ntime: 7417s (wall 8249s)\nstep 842184 / 1559752 (epoch 107.99 / 200):\n  learning_rate = 3.42e-04, loss_average = 2.2886e-05\nvalidation loss: 7.8416e-04\ntime: 7491s (wall 8326s)\nstep 849982 / 1559752 (epoch 108.99 / 200):\n  learning_rate = 3.38e-04, loss_average = 2.2167e-05\nvalidation loss: 7.5583e-04\ntime: 7566s (wall 8405s)\nstep 857780 / 1559752 (epoch 109.99 / 200):\n  learning_rate = 3.35e-04, loss_average = 2.0416e-05\nvalidation loss: 7.6639e-04\ntime: 7643s (wall 8486s)\nstep 865578 / 1559752 (epoch 110.99 / 200):\n  learning_rate = 3.31e-04, loss_average = 2.0814e-05\nvalidation loss: 7.6148e-04\ntime: 7717s (wall 8563s)\nstep 873376 / 1559752 (epoch 111.99 / 200):\n  learning_rate = 3.28e-04, loss_average = 2.1530e-05\nvalidation loss: 7.5688e-04\ntime: 7791s (wall 8641s)\nstep 881174 / 1559752 (epoch 112.99 / 200):\n  learning_rate = 3.25e-04, loss_average = 2.1490e-05\nvalidation loss: 7.6799e-04\ntime: 7867s (wall 8720s)\nstep 888972 / 1559752 (epoch 113.99 / 200):\n  learning_rate = 3.22e-04, loss_average = 2.0320e-05\nvalidation loss: 7.5424e-04\ntime: 7942s (wall 8798s)\nstep 896770 / 1559752 (epoch 114.99 / 200):\n  learning_rate = 3.19e-04, loss_average = 1.9828e-05\nvalidation loss: 7.5473e-04\ntime: 8019s (wall 8879s)\nstep 904568 / 1559752 (epoch 115.99 / 200):\n  learning_rate = 3.15e-04, loss_average = 2.0164e-05\nvalidation loss: 7.4517e-04\ntime: 8094s (wall 8956s)\nstep 912366 / 1559752 (epoch 116.99 / 200):\n  learning_rate = 3.12e-04, loss_average = 2.0390e-05\nvalidation loss: 7.5547e-04\ntime: 8173s (wall 9038s)\nstep 920164 / 1559752 (epoch 117.99 / 200):\n  learning_rate = 3.09e-04, loss_average = 2.0434e-05\nvalidation loss: 7.4661e-04\ntime: 8249s (wall 9117s)\nstep 927962 / 1559752 (epoch 118.99 / 200):\n  learning_rate = 3.06e-04, loss_average = 1.9343e-05\nvalidation loss: 7.5159e-04\ntime: 8324s (wall 9195s)\nstep 935760 / 1559752 (epoch 119.99 / 200):\n  learning_rate = 3.03e-04, loss_average = 2.0020e-05\nvalidation loss: 7.4167e-04\ntime: 8401s (wall 9275s)\nstep 943558 / 1559752 (epoch 120.99 / 200):\n  learning_rate = 3.00e-04, loss_average = 1.9714e-05\nvalidation loss: 7.4243e-04\ntime: 8480s (wall 9357s)\nstep 951356 / 1559752 (epoch 121.99 / 200):\n  learning_rate = 2.97e-04, loss_average = 2.0389e-05\nvalidation loss: 7.3716e-04\ntime: 8554s (wall 9434s)\nstep 959154 / 1559752 (epoch 122.99 / 200):\n  learning_rate = 2.94e-04, loss_average = 2.0362e-05\nvalidation loss: 7.4303e-04\ntime: 8632s (wall 9516s)\nstep 966952 / 1559752 (epoch 123.99 / 200):\n  learning_rate = 2.91e-04, loss_average = 2.0819e-05\nvalidation loss: 7.3951e-04\ntime: 8707s (wall 9593s)\nstep 974750 / 1559752 (epoch 124.99 / 200):\n  learning_rate = 2.88e-04, loss_average = 2.0030e-05\nvalidation loss: 7.4457e-04\ntime: 8777s (wall 9670s)\nstep 982548 / 1559752 (epoch 125.99 / 200):\n  learning_rate = 2.86e-04, loss_average = 1.8964e-05\nvalidation loss: 7.3144e-04\ntime: 8845s (wall 9746s)\nstep 990346 / 1559752 (epoch 126.99 / 200):\n  learning_rate = 2.83e-04, loss_average = 1.9182e-05\nvalidation loss: 7.2513e-04\ntime: 8918s (wall 9825s)\nstep 998144 / 1559752 (epoch 127.99 / 200):\n  learning_rate = 2.80e-04, loss_average = 1.8953e-05\nvalidation loss: 7.3066e-04\ntime: 8989s (wall 9904s)\nstep 1005942 / 1559752 (epoch 128.99 / 200):\n  learning_rate = 2.77e-04, loss_average = 1.9743e-05\nvalidation loss: 7.3349e-04\ntime: 9061s (wall 9981s)\nstep 1013740 / 1559752 (epoch 129.99 / 200):\n  learning_rate = 2.74e-04, loss_average = 1.7787e-05\nvalidation loss: 7.1947e-04\ntime: 9138s (wall 10060s)\nstep 1021538 / 1559752 (epoch 130.99 / 200):\n  learning_rate = 2.72e-04, loss_average = 1.9202e-05\nvalidation loss: 7.2155e-04\ntime: 9216s (wall 10142s)\nstep 1029336 / 1559752 (epoch 131.99 / 200):\n  learning_rate = 2.69e-04, loss_average = 1.7506e-05\nvalidation loss: 7.1332e-04\ntime: 9290s (wall 10219s)\nstep 1037134 / 1559752 (epoch 132.99 / 200):\n  learning_rate = 2.66e-04, loss_average = 1.9624e-05\nvalidation loss: 7.1594e-04\ntime: 9368s (wall 10301s)\nstep 1044932 / 1559752 (epoch 133.99 / 200):\n  learning_rate = 2.64e-04, loss_average = 1.7932e-05\nvalidation loss: 7.1489e-04\ntime: 9445s (wall 10380s)\nstep 1052730 / 1559752 (epoch 134.99 / 200):\n  learning_rate = 2.61e-04, loss_average = 1.8550e-05\nvalidation loss: 7.2072e-04\ntime: 9520s (wall 10459s)\nstep 1060528 / 1559752 (epoch 135.99 / 200):\n  learning_rate = 2.58e-04, loss_average = 1.8511e-05\nvalidation loss: 7.1252e-04\ntime: 9595s (wall 10537s)\nstep 1068326 / 1559752 (epoch 136.99 / 200):\n  learning_rate = 2.56e-04, loss_average = 1.7807e-05\nvalidation loss: 7.0809e-04\ntime: 9674s (wall 10618s)\nstep 1076124 / 1559752 (epoch 137.99 / 200):\n  learning_rate = 2.53e-04, loss_average = 1.8687e-05\nvalidation loss: 7.1057e-04\ntime: 9752s (wall 10700s)\nstep 1083922 / 1559752 (epoch 138.99 / 200):\n  learning_rate = 2.51e-04, loss_average = 1.7796e-05\nvalidation loss: 7.1235e-04\ntime: 9828s (wall 10779s)\nstep 1091720 / 1559752 (epoch 139.99 / 200):\n  learning_rate = 2.48e-04, loss_average = 1.7501e-05\nvalidation loss: 7.0606e-04\ntime: 9903s (wall 10857s)\nstep 1099518 / 1559752 (epoch 140.99 / 200):\n  learning_rate = 2.46e-04, loss_average = 1.7329e-05\nvalidation loss: 7.0052e-04\ntime: 9981s (wall 10939s)\nstep 1107316 / 1559752 (epoch 141.99 / 200):\n  learning_rate = 2.44e-04, loss_average = 1.7837e-05\nvalidation loss: 6.9882e-04\ntime: 10059s (wall 11020s)\nstep 1115114 / 1559752 (epoch 142.99 / 200):\n  learning_rate = 2.41e-04, loss_average = 1.7317e-05\nvalidation loss: 7.0418e-04\ntime: 10134s (wall 11098s)\nstep 1122912 / 1559752 (epoch 143.99 / 200):\n  learning_rate = 2.39e-04, loss_average = 1.7471e-05\nvalidation loss: 6.9912e-04\ntime: 10205s (wall 11176s)\nstep 1130710 / 1559752 (epoch 144.99 / 200):\n  learning_rate = 2.36e-04, loss_average = 1.6816e-05\nvalidation loss: 6.9764e-04\ntime: 10274s (wall 11252s)\nstep 1138508 / 1559752 (epoch 145.99 / 200):\n  learning_rate = 2.34e-04, loss_average = 1.6571e-05\nvalidation loss: 6.9983e-04\ntime: 10346s (wall 11331s)\nstep 1146306 / 1559752 (epoch 146.99 / 200):\n  learning_rate = 2.32e-04, loss_average = 1.7024e-05\nvalidation loss: 7.0115e-04\ntime: 10421s (wall 11410s)\nstep 1154104 / 1559752 (epoch 147.99 / 200):\n  learning_rate = 2.29e-04, loss_average = 1.6546e-05\nvalidation loss: 6.9151e-04\ntime: 10496s (wall 11488s)\nstep 1161902 / 1559752 (epoch 148.99 / 200):\n  learning_rate = 2.27e-04, loss_average = 1.6783e-05\nvalidation loss: 6.9921e-04\ntime: 10574s (wall 11569s)\nstep 1169700 / 1559752 (epoch 149.99 / 200):\n  learning_rate = 2.25e-04, loss_average = 1.5700e-05\nvalidation loss: 6.8579e-04\ntime: 10649s (wall 11647s)\nstep 1177498 / 1559752 (epoch 150.99 / 200):\n  learning_rate = 2.23e-04, loss_average = 1.6724e-05\nvalidation loss: 6.9294e-04\ntime: 10728s (wall 11729s)\nstep 1185296 / 1559752 (epoch 151.99 / 200):\n  learning_rate = 2.20e-04, loss_average = 1.7115e-05\nvalidation loss: 6.9072e-04\ntime: 10804s (wall 11809s)\nstep 1193094 / 1559752 (epoch 152.99 / 200):\n  learning_rate = 2.18e-04, loss_average = 1.5805e-05\nvalidation loss: 6.9101e-04\ntime: 10879s (wall 11886s)\nstep 1200892 / 1559752 (epoch 153.98 / 200):\n  learning_rate = 2.16e-04, loss_average = 1.6514e-05\nvalidation loss: 6.8203e-04\ntime: 10956s (wall 11966s)\nstep 1208690 / 1559752 (epoch 154.98 / 200):\n  learning_rate = 2.14e-04, loss_average = 1.6782e-05\nvalidation loss: 6.8227e-04\ntime: 11032s (wall 12046s)\nstep 1216488 / 1559752 (epoch 155.98 / 200):\n  learning_rate = 2.12e-04, loss_average = 1.6338e-05\nvalidation loss: 6.8178e-04\ntime: 11108s (wall 12125s)\nstep 1224286 / 1559752 (epoch 156.98 / 200):\n  learning_rate = 2.10e-04, loss_average = 1.6451e-05\nvalidation loss: 6.8128e-04\ntime: 11186s (wall 12206s)\nstep 1232084 / 1559752 (epoch 157.98 / 200):\n  learning_rate = 2.08e-04, loss_average = 1.6525e-05\nvalidation loss: 6.7809e-04\ntime: 11263s (wall 12287s)\nstep 1239882 / 1559752 (epoch 158.98 / 200):\n  learning_rate = 2.06e-04, loss_average = 1.6341e-05\nvalidation loss: 6.7633e-04\ntime: 11342s (wall 12369s)\nstep 1247680 / 1559752 (epoch 159.98 / 200):\n  learning_rate = 2.04e-04, loss_average = 1.6986e-05\nvalidation loss: 6.7171e-04\ntime: 11421s (wall 12450s)\nstep 1255478 / 1559752 (epoch 160.98 / 200):\n  learning_rate = 2.02e-04, loss_average = 1.5849e-05\nvalidation loss: 6.7472e-04\ntime: 11502s (wall 12533s)\nstep 1263276 / 1559752 (epoch 161.98 / 200):\n  learning_rate = 2.00e-04, loss_average = 1.6649e-05\nvalidation loss: 6.7088e-04\ntime: 11579s (wall 12613s)\nstep 1271074 / 1559752 (epoch 162.98 / 200):\n  learning_rate = 1.98e-04, loss_average = 1.5968e-05\nvalidation loss: 6.6436e-04\ntime: 11657s (wall 12694s)\nstep 1278872 / 1559752 (epoch 163.98 / 200):\n  learning_rate = 1.96e-04, loss_average = 1.5580e-05\nvalidation loss: 6.6734e-04\ntime: 11731s (wall 12773s)\nstep 1286670 / 1559752 (epoch 164.98 / 200):\n  learning_rate = 1.94e-04, loss_average = 1.5626e-05\nvalidation loss: 6.6631e-04\ntime: 11804s (wall 12851s)\nstep 1294468 / 1559752 (epoch 165.98 / 200):\n  learning_rate = 1.92e-04, loss_average = 1.5405e-05\nvalidation loss: 6.6161e-04\ntime: 11876s (wall 12928s)\nstep 1302266 / 1559752 (epoch 166.98 / 200):\n  learning_rate = 1.90e-04, loss_average = 1.5505e-05\nvalidation loss: 6.6553e-04\ntime: 11953s (wall 13009s)\nstep 1310064 / 1559752 (epoch 167.98 / 200):\n  learning_rate = 1.88e-04, loss_average = 1.5405e-05\nvalidation loss: 6.6453e-04\ntime: 12028s (wall 13088s)\nstep 1317862 / 1559752 (epoch 168.98 / 200):\n  learning_rate = 1.86e-04, loss_average = 1.5542e-05\nvalidation loss: 6.5991e-04\ntime: 12098s (wall 13164s)\nstep 1325660 / 1559752 (epoch 169.98 / 200):\n  learning_rate = 1.84e-04, loss_average = 1.5154e-05\nvalidation loss: 6.7091e-04\ntime: 12169s (wall 13242s)\nstep 1333458 / 1559752 (epoch 170.98 / 200):\n  learning_rate = 1.82e-04, loss_average = 1.4866e-05\nvalidation loss: 6.5857e-04\ntime: 12241s (wall 13319s)\nstep 1341256 / 1559752 (epoch 171.98 / 200):\n  learning_rate = 1.81e-04, loss_average = 1.4305e-05\nvalidation loss: 6.5583e-04\ntime: 12317s (wall 13399s)\nstep 1349054 / 1559752 (epoch 172.98 / 200):\n  learning_rate = 1.79e-04, loss_average = 1.4823e-05\nvalidation loss: 6.6083e-04\ntime: 12393s (wall 13479s)\nstep 1356852 / 1559752 (epoch 173.98 / 200):\n  learning_rate = 1.77e-04, loss_average = 1.5707e-05\nvalidation loss: 6.5839e-04\ntime: 12464s (wall 13555s)\nstep 1364650 / 1559752 (epoch 174.98 / 200):\n  learning_rate = 1.75e-04, loss_average = 1.5342e-05\nvalidation loss: 6.5162e-04\ntime: 12541s (wall 13635s)\nstep 1372448 / 1559752 (epoch 175.98 / 200):\n  learning_rate = 1.74e-04, loss_average = 1.4906e-05\nvalidation loss: 6.5305e-04\ntime: 12619s (wall 13716s)\nstep 1380246 / 1559752 (epoch 176.98 / 200):\n  learning_rate = 1.72e-04, loss_average = 1.4748e-05\nvalidation loss: 6.5426e-04\ntime: 12687s (wall 13792s)\nstep 1388044 / 1559752 (epoch 177.98 / 200):\n  learning_rate = 1.70e-04, loss_average = 1.4623e-05\nvalidation loss: 6.5261e-04\ntime: 12759s (wall 13870s)\nstep 1395842 / 1559752 (epoch 178.98 / 200):\n  learning_rate = 1.69e-04, loss_average = 1.4994e-05\nvalidation loss: 6.4541e-04\ntime: 12829s (wall 13946s)\nstep 1403640 / 1559752 (epoch 179.98 / 200):\n  learning_rate = 1.67e-04, loss_average = 1.5289e-05\nvalidation loss: 6.4949e-04\ntime: 12901s (wall 14024s)\nstep 1411438 / 1559752 (epoch 180.98 / 200):\n  learning_rate = 1.65e-04, loss_average = 1.5117e-05\nvalidation loss: 6.3953e-04\ntime: 12971s (wall 14101s)\nstep 1419236 / 1559752 (epoch 181.98 / 200):\n  learning_rate = 1.64e-04, loss_average = 1.4513e-05\nvalidation loss: 6.4382e-04\ntime: 13045s (wall 14180s)\nstep 1427034 / 1559752 (epoch 182.98 / 200):\n  learning_rate = 1.62e-04, loss_average = 1.4443e-05\nvalidation loss: 6.3659e-04\ntime: 13114s (wall 14257s)\nstep 1434832 / 1559752 (epoch 183.98 / 200):\n  learning_rate = 1.60e-04, loss_average = 1.4025e-05\nvalidation loss: 6.4708e-04\ntime: 13181s (wall 14333s)\nstep 1442630 / 1559752 (epoch 184.98 / 200):\n  learning_rate = 1.59e-04, loss_average = 1.4851e-05\nvalidation loss: 6.3830e-04\ntime: 13248s (wall 14408s)\nstep 1450428 / 1559752 (epoch 185.98 / 200):\n  learning_rate = 1.57e-04, loss_average = 1.4608e-05\nvalidation loss: 6.4423e-04\ntime: 13313s (wall 14483s)\nstep 1458226 / 1559752 (epoch 186.98 / 200):\n  learning_rate = 1.56e-04, loss_average = 1.4312e-05\nvalidation loss: 6.3741e-04\ntime: 13380s (wall 14559s)\nstep 1466024 / 1559752 (epoch 187.98 / 200):\n  learning_rate = 1.54e-04, loss_average = 1.4548e-05\nvalidation loss: 6.4021e-04\ntime: 13445s (wall 14633s)\nstep 1473822 / 1559752 (epoch 188.98 / 200):\n  learning_rate = 1.53e-04, loss_average = 1.3352e-05\nvalidation loss: 6.3891e-04\ntime: 13512s (wall 14709s)\nstep 1481620 / 1559752 (epoch 189.98 / 200):\n  learning_rate = 1.51e-04, loss_average = 1.4892e-05\nvalidation loss: 6.3123e-04\ntime: 13577s (wall 14783s)\nstep 1489418 / 1559752 (epoch 190.98 / 200):\n  learning_rate = 1.50e-04, loss_average = 1.3997e-05\nvalidation loss: 6.3465e-04\ntime: 13644s (wall 14859s)\nstep 1497216 / 1559752 (epoch 191.98 / 200):\n  learning_rate = 1.48e-04, loss_average = 1.4615e-05\nvalidation loss: 6.3437e-04\ntime: 13709s (wall 14933s)\nstep 1505014 / 1559752 (epoch 192.98 / 200):\n  learning_rate = 1.47e-04, loss_average = 1.4139e-05\nvalidation loss: 6.3238e-04\ntime: 13776s (wall 15009s)\nstep 1512812 / 1559752 (epoch 193.98 / 200):\n  learning_rate = 1.45e-04, loss_average = 1.3837e-05\nvalidation loss: 6.2866e-04\ntime: 13841s (wall 15083s)\nstep 1520610 / 1559752 (epoch 194.98 / 200):\n  learning_rate = 1.44e-04, loss_average = 1.4305e-05\nvalidation loss: 6.3323e-04\ntime: 13909s (wall 15160s)\nstep 1528408 / 1559752 (epoch 195.98 / 200):\n  learning_rate = 1.42e-04, loss_average = 1.3698e-05\nvalidation loss: 6.3023e-04\ntime: 13976s (wall 15236s)\nstep 1536206 / 1559752 (epoch 196.98 / 200):\n  learning_rate = 1.41e-04, loss_average = 1.3618e-05\nvalidation loss: 6.3003e-04\ntime: 14041s (wall 15310s)\nstep 1544004 / 1559752 (epoch 197.98 / 200):\n  learning_rate = 1.40e-04, loss_average = 1.3107e-05\nvalidation loss: 6.2515e-04\ntime: 14106s (wall 15384s)\nstep 1551802 / 1559752 (epoch 198.98 / 200):\n  learning_rate = 1.38e-04, loss_average = 1.3414e-05\nvalidation loss: 6.2596e-04\ntime: 14174s (wall 15461s)\nstep 1559600 / 1559752 (epoch 199.98 / 200):\n  learning_rate = 1.37e-04, loss_average = 1.3471e-05\nvalidation loss: 6.2888e-04\ntime: 14241s (wall 15537s)\nvalidation loss: trough = 0.0006, mean = 0.00\nFinal training error:  [1755948901.871655, 1559600, 1.347088e-05]\nFinal validation error:  [0, 1559600, 0.0006288782131166578]\n[0.0020261846955951274, 0.0022004475588459174, 0.002111136113861097, 0.0019356121879407701, 0.0018134365513509383, 0.0017709666269867155, 0.0016346190211626293, 0.001629534170856607, 0.0015021155760311059, 0.0015445760893422712, 0.0014544676257751822, 0.0014612884402167335, 0.0014061500549197453, 0.0013852047158574078, 0.0013621169960355021, 0.0013262529101547677, 0.0012922427733627962, 0.0012929263086909272, 0.001269548175014417, 0.0012546321812165252, 0.0011902070250275254, 0.0012364912620455206, 0.0011976752617138219, 0.0011925882229837403, 0.0011526465483918287, 0.0011639408478156385, 0.001139133268900073, 0.0010907299566621043, 0.0011236106415226247, 0.0011178811543422157, 0.001076794715198226, 0.0010653447329158458, 0.001093928535103757, 0.0010928587282314245, 0.0010982531471218052, 0.0010663940815978008, 0.0010751083975607206, 0.0010382515027099613, 0.0010520011016113464, 0.001071081752914732, 0.001020260170471588, 0.0010303816914639424, 0.0010164625448079528, 0.0010307156496686177, 0.000992321129898666, 0.001007771472890901, 0.0010098638096904908, 0.000992133013669728, 0.0010090190747077301, 0.0009834647897380834, 0.0009734377967647244, 0.0009946775742518713, 0.0009757131198067718, 0.0009554818218429339, 0.0009569139055702042, 0.0009701017090385838, 0.0009670374600985089, 0.0009684440616644441, 0.0009510696792128645, 0.0009595966811220717, 0.0009117703627571021, 0.0009406090730556558, 0.0009437753341813652, 0.0008938571839317126, 0.0009290848109285795, 0.0008973983400338101, 0.0009260825737565158, 0.0008902709510108114, 0.0008981306252148518, 0.0009080165510436151, 0.0008860359646318943, 0.0008863272121850557, 0.000873531653707598, 0.0008618422339933558, 0.0008828763003838335, 0.0008760315701396355, 0.0008899843762878026, 0.0008822496234713115, 0.0008504775968071644, 0.0008620120248813765, 0.0008783150661103987, 0.0008607381733084219, 0.0008571182342020022, 0.0008304692636544865, 0.0008294665051268616, 0.0008411858386166032, 0.0008282663060558145, 0.0008189693081208008, 0.0008312497910963208, 0.0008216365328418195, 0.0008119291987552534, 0.0008162582883184561, 0.0008169677140300598, 0.0008172639416509898, 0.0008098187545212834, 0.0008038854591666287, 0.0008234542792055298, 0.0007929705687545236, 0.000797457655408763, 0.0007826811108033442, 0.000806438475679024, 0.0007910069012743419, 0.0007904570188114524, 0.0007797791094601156, 0.0007870660437262612, 0.0007679015586061762, 0.0007883928511053375, 0.0007841565991959546, 0.0007558266376377021, 0.0007663942346776226, 0.0007614847962538826, 0.0007568814943422656, 0.0007679866018525408, 0.0007542374702387913, 0.0007547269831988771, 0.0007451697862106274, 0.0007554663464677296, 0.0007466108508197901, 0.0007515866272467582, 0.0007416656212685265, 0.0007424255810828656, 0.000737160979981734, 0.0007430344032302247, 0.0007395144167123879, 0.0007445738535251121, 0.0007314435572892928, 0.0007251324706645755, 0.0007306557335733254, 0.0007334903844063011, 0.0007194668174578288, 0.0007215493330288603, 0.0007133203236184849, 0.0007159428454678563, 0.0007148860720826203, 0.0007207208418050021, 0.0007125151671859018, 0.000708090235462469, 0.0007105690093570125, 0.000712354291278787, 0.0007060584310289562, 0.0007005157951887291, 0.0006988224685318766, 0.0007041796355229062, 0.0006991166060232229, 0.0006976383564603982, 0.0006998277066707421, 0.0007011538465385895, 0.000691511223493529, 0.0006992055620771203, 0.0006857852063257808, 0.0006929352181917345, 0.0006907170596170333, 0.0006910118499064958, 0.0006820256952935527, 0.0006822740275823147, 0.0006817802828851351, 0.0006812784548457193, 0.0006780939934623679, 0.0006763321301609313, 0.0006717071274567469, 0.0006747158871759785, 0.0006708782187187993, 0.0006643604088266439, 0.000667338071897212, 0.0006663051595973472, 0.0006616124179703409, 0.000665532883655125, 0.0006645321474669051, 0.0006599090389591799, 0.0006709089830492774, 0.0006585675633332324, 0.0006558321381116861, 0.0006608257218635262, 0.0006583900511002196, 0.0006516185759370437, 0.0006530523860768725, 0.0006542568698414381, 0.0006526064098857028, 0.0006454052622657017, 0.0006494876709946283, 0.0006395324661623533, 0.0006438169215796995, 0.0006365867856125448, 0.0006470784042158791, 0.0006383002712948628, 0.0006442282958476146, 0.000637406100584807, 0.0006402128548903662, 0.0006389095368698193, 0.0006312283264549765, 0.000634645762413545, 0.0006343722921596739, 0.0006323813611123884, 0.0006286563325119948, 0.000633226194132941, 0.0006302313321614136, 0.0006300255783525683, 0.0006251452887636471, 0.0006259625027190842, 0.0006288782131166578]\n0.009963054560046803\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"/kaggle/working/lcn-pose/output\", 'zip', \"/kaggle/working/lcn-pose/experiment\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T11:35:17.924176Z","iopub.execute_input":"2025-08-23T11:35:17.924606Z","iopub.status.idle":"2025-08-23T11:35:25.917457Z","shell.execute_reply.started":"2025-08-23T11:35:17.924573Z","shell.execute_reply":"2025-08-23T11:35:25.916811Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/lcn-pose/output.zip'"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"!python inference.py --train_set={TRAIN_SET} --test_set={TEST_SET} --mask-type {MASK_TYPE} --channels {F} --epoch={EPOCH} --dropout {DROPOUT} --layers {LAYERS} --batch_size {BATCH_SIZE} --knn {KNN} --learning_rate {LEARNING_RATE} --test-indices {TEST_INDICES} --regularization {REGULARIZATION} --checkpoints best","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python evaluate.py --test-indices {TEST_INDICES} --per-joint --filename {TEST_SET}","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}